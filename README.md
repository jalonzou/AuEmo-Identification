# Audio Emotion Identification System (AEIS)
Audio Emotion Identification System (AEIS) is a smart audio feature analyzer. Essentially it was designed for ehancing semantic analyzing ability of Virtual Assistant. AEIS is a low-level subsystem behaving as an augmented information provider embedded in an intergrated smart home solution. 

We do not prescribe any kind of potential higher-level application of this system, which means that you are free to produce and input any sorts of audio data into this system and let it preprocess, analyze and output their corresponding emotion-related information. Although under certain scenarios its accuracy prevails over others.

## Workflow Overview
The general cycle for analyzing an audio data file in this system can be summarized into following steps:

1. Decode data file into sound wave information stored as an one-dimensional array.
2. Preprocess sound wave info. (Amplification, Weakening, Standarization, etc.)
3. Convert data from Time-Amplitude space to Time-Frequency space (FFT), generate spectrogram.
4. Transform spectrogram into mel-spectrogram with log freqency scale.
5. Preprocess the resulting mel-spectrogram. (Brightness, Contrast, and Per-Image Standarization)
6. Auto crop the image data. (Extract typical fragment)
7. Input resulting data into Neural Network.
8. Output final result.

The whole processing and analyzing process has been encapsulated into system interfaces. So you are not required to be aware of the technical details of this system. In certain case, however, you ask for a more flexible handling of each step. In future release, we will implement an extra layer for users to visulize the status of this system in real time.

## Interface Shortcut
Before your invoke any interface of this system, make sure you have pulled this repository from github and `cd` into the root directory.

### Raw audio to mel-spectrogram

The following interface may help you convert from raw audio data into preprocessed mel-spectrogram:
```
cvtRawToSpec(file_path, output_Path, file_type)
```
Note that you should prepare you data file under specific directory. An example of this interface:
```
cvtRawToSpec('/home/my_audio', '/home/output', '.mp3')
```

### Train using self-generated dataset

The following interface will trigger the training process of the system
```
trainAEIS(dataset_path, epoch, log_mode)
```
Note that your dataset format should conform with our support formats. They can be either generated by our data assembling module or by your customized method. An example of this interface:
```
trainAEIS('/home/my_dataset', 1000, 2)
```

## Audio Data Processing
In this section, we present some technical datails on our implementation of audio data converting process.

## Dataset generation
In this section, we demonstrate our way in assembling training dataset for neural network training purpose.

## Network Structure
In this section, we present our idea of designing an expressive neural network structure which is complex enough to extract as much audio feature as possible from input dataset while keeping its degree of complexity under certain threshold so as to control potential overfiting problems.

Our training module is built on Tensorflow framework, one of the most widely accepted frameworks in deep learning field. With the help of this framework, we can implement our network structure in a few lines of code and conduct training process on it in a systematic way. All the computation units inside the network, the hidden and output layer, the accuracy calculator, the logger,etc. are defined as nodes in Tensorflow, which is extremely straightforward for you to define your own operation.

The current release of our system is based on Tensorflow 1.8. For more information of this version, see: https://www.tensorflow.org/api_docs/[https://www.tensorflow.org/api_docs/]

### Local Feature extraction Layer
The 

Implementation of Audio emotion identification neural network using CNN and RNN
